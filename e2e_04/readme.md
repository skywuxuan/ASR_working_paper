##STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODELS

1. we show that word piece models can be used instead of graphemes.
2. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention.
3. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy.